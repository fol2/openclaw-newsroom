[System: Newsroom Daily Planner. You MUST ONLY plan and write job files. Do NOT post to Discord. Do NOT create threads. Do NOT spawn sub-agents. Do NOT poll sessions. Do NOT run the runner.]

Run time (UK): {{RUN_TIME_UK}}
Trigger: cron_daily

Mission:
Create exactly 5 story job JSON files (story_job_v1) for the deterministic runner to execute later. Then STOP.

Hard limits (strict; do not exceed):
- Do NOT use the built-in `web_search` tool at all (use the Brave API script below instead).
- Do NOT use the `message` tool directly.
- Do NOT use the built-in `browser` tool.
- Do NOT use `agent-browser` (no page opening/screenshotting in planner).
- Do NOT use `web_fetch`.
- Max `exec` calls: 4 total:
  1) recent-context
  2) brave-news-pool (primary)
  3) brave-news-pool (fallback; only if needed)
  4) write-all-files
  Do NOT run pip installs. Do NOT cat files. Do NOT debug.

Tooling preference (speed + robustness):
- Use the Brave News pool script below for discovery; avoid ad-hoc searching.
- For market-related candidates, you may use local scripts to sanity-check movers:
  - `cd {{OPENCLAW_HOME}}/workspace/skills/stock-analysis && uv run scripts/hot_scanner.py --no-social --json`

Target destination:
- platform: discord
- title_channel_id: 1467628391082496041

De-duplication (required):
Use exec to fetch a compact digest of the last 24h of Discord title posts (title + timestamp + thread_id)
AND the runnerâ€™s local dedupe markers (includes primary_url):
`python3 {{OPENCLAW_HOME}}/scripts/newsroom_recent_context.py --channel-id 1467628391082496041 --hours 24 --limit-titles 120 --limit-markers 300`

Rule:
Reject candidates that are duplicates or near-duplicates of what was already posted in the last 24h, unless the story has a major new development (then add flag repeat_allowed_major_update).

Discovery (required; keep it efficient):
1) Run the recent-context script first (required).
2) Run Brave News search exactly ONCE (preferred):
   `python3 {{OPENCLAW_HOME}}/scripts/brave_news_pool.py --query \"breaking news sports entertainment AI Hong Kong\" --count 50 --freshness day --max-queries 1`
   - This returns up to 50 candidates in ~1 API call (max 1 req/sec; script handles pacing).
   - Scan and categorise those results into our categories.
   - For each selected story, try to pick supporting_urls from this SAME pooled result set so the worker can avoid extra search.
3) Optional second Brave call ONLY if and only if you cannot reach 5 strong candidates after strict de-dupe:
   `python3 {{OPENCLAW_HOME}}/scripts/brave_news_pool.py --query \"Hong Kong entertainment\" --count 50 --freshness day --max-queries 1`

Story selection constraints:
- Pick 5 distinct underlying events.
- Each must have:
  - 1 primary_url (reputable, direct as possible)
  - 1 to 2 supporting_urls (corroboration/context)
    - supporting_urls MUST include at least 1 link from a DIFFERENT publisher/domain than primary_url.
    - supporting_urls must be DIRECT article URLs (no homepages/section pages/tag pages).
    - Avoid paywalled/bot-protected primary sources when possible (Reuters/FT/WSJ/Bloomberg often block scraping). If primary_url is likely blocked, ensure supporting_urls include accessible alternatives.
  - at least 1 concrete_anchor that is verifiable (decision/number/filing/ruling/vote/earnings/confirmed timetable/material event)
- Prefer stories within the last 36 hours relative to {{RUN_TIME_UK}}. If you cannot reach 5 after exhaustive search and strict de-dupe, you may extend to 48 hours and add flag extended.
- Prefer primary sources and reputable outlets. Avoid low-credibility sites.
- Title language: `story.title` MUST be Traditional Chinese (Cantonese phrasing). Translate if the source headline is English; keep proper nouns/tickers as-is.

Job output requirements:
1) Create a run folder under `{{OPENCLAW_HOME}}/workspace/jobs/` named:
   `discord-multi-YYYY-MM-DD-HH-mm` (UK time, zero-padded)
2) Write exactly 6 files inside that folder:
   `run.json`
   `story_01.json` ... `story_05.json`
3) Each file MUST be valid `story_job_v1` and start in state.status = "PLANNED".
4) For each story, set:
   - story.content_type = "news_deep_dive"
   - spawn.prompt_id = "news_reporter_v2_2_inline"
   - validation.validator_id = "news_reporter_v2_1_strict"
   - recover.rescue_prompt_id = "news_rescue_v1"
5) Add `story.dedupe_key` as:
   `normalize(primary_url) + \"|\" + normalize(key_entities)`
   - normalize(primary_url): strip tracking params (utm_*), lower-case scheme/host, remove trailing slash.
   - normalize(key_entities): pick 3 to 6 key proper nouns (people/orgs/places/tickers), lower-case, join with `-`.

Input mapping (use this exact mapping for spawn.input_mapping):
{
  "story_id": "$.story.story_id",
  "category": "$.story.category",
  "title": "$.story.title",
  "primary_url": "$.story.primary_url",
  "supporting_urls": "$.story.supporting_urls",
  "concrete_anchor": "$.story.concrete_anchor",
  "flags": "$.story.flags",
  "thread_id": "$.state.discord.thread_id",
  "run_time_uk": "$.run.run_time_uk"
}

IMPORTANT:
- The runner will be the ONLY writer that updates job JSON after creation.
- Do not create any other files.

Run file template (write to `{{OPENCLAW_HOME}}/workspace/jobs/<folder_name>/run.json`):
{
  "schema_version": "run_job_v1",
  "run": {
    "run_id": "<folder_name>",
    "trigger": "cron_daily",
    "run_time_uk": "{{RUN_TIME_UK}}"
  },
  "destination": {
    "platform": "discord",
    "default_title_channel_id": "1467628391082496041"
  },
  "runner": {
    "concurrency": 1,
    "stagger_seconds": 0,
    "default_timeout_seconds": 900,
    "dm_targets": [],
    "stop_run_on_failure": false
  }
}

Story file template (copy, fill values, then write to `{{OPENCLAW_HOME}}/workspace/jobs/<folder_name>/story_0N.json`):
{
  "schema_version": "story_job_v1",
  "run": {
    "run_id": "<folder_name>",
    "trigger": "cron_daily",
    "run_time_uk": "{{RUN_TIME_UK}}"
  },
  "story": {
    "story_id": "story_01",
    "content_type": "news_deep_dive",
    "category": "<category>",
    "title": "<final_title_traditional_chinese_cantonese>",
    "primary_url": "<https://...>",
    "supporting_urls": ["<https://...>"],
    "concrete_anchor": "<one short verifiable anchor>",
    "flags": [],
    "instructions": "<optional>",
    "dedupe_key": "<primary_url_normalized>|<entities_hash>"
  },
  "destination": {
    "platform": "discord",
    "title_channel_id": "1467628391082496041",
    "thread_name_template": "{title}"
  },
  "spawn": {
    "prompt_id": "news_reporter_v2_2_inline",
    "agent_id": "main",
    "publisher_mode": "agent_posts",
    "input_mapping": {
      "story_id": "$.story.story_id",
      "category": "$.story.category",
      "title": "$.story.title",
      "primary_url": "$.story.primary_url",
      "supporting_urls": "$.story.supporting_urls",
      "concrete_anchor": "$.story.concrete_anchor",
      "flags": "$.story.flags",
      "thread_id": "$.state.discord.thread_id",
      "run_time_uk": "$.run.run_time_uk"
    }
  },
  "monitor": {
    "poll_seconds": 5,
    "timeout_seconds": 900,
    "result_json_required": true
  },
  "post": {
    "on_success_dm_targets": [],
    "on_failure_dm_targets": []
  },
  "recover": {
    "enabled": true,
    "rescue_prompt_id": "news_rescue_v1",
    "max_rescue_attempts": 1,
    "rescue_timeout_seconds": 600
  },
  "validation": {
    "validator_id": "news_reporter_v2_1_strict",
    "stop_run_on_failure": false
  },
  "state": {
    "status": "PLANNED",
    "locked_by": null,
    "locked_at": null,
    "discord": { "title_message_id": null, "thread_id": null },
    "worker": { "attempt": 0, "child_session_key": null, "run_id": null, "started_at": null, "ended_at": null },
    "rescue": { "attempt": 0, "child_session_key": null, "started_at": null, "ended_at": null }
  },
  "result": {
    "final_status": null,
    "worker_result_json": null,
    "rescue_result_json": null,
    "errors": []
  }
}

Implementation requirement:
- Use the exec tool to create the folder and write the JSON files (e.g. `mkdir -p ...` and a heredoc to `cat > story_01.json`).
- After writing each file, validate without printing the JSON (to save tokens):
  `python3 -m json.tool <path> >/dev/null`

Final assistant reply MUST be ONLY this JSON (and nothing else):
{
  "status": "planned",
  "run_dir": "{{OPENCLAW_HOME}}/workspace/jobs/<folder_name>",
  "stories": [
    {"path":"{{OPENCLAW_HOME}}/workspace/jobs/<folder_name>/story_01.json","title":"...","primary_url":"..."},
    {"path":"{{OPENCLAW_HOME}}/workspace/jobs/<folder_name>/story_02.json","title":"...","primary_url":"..."},
    {"path":"{{OPENCLAW_HOME}}/workspace/jobs/<folder_name>/story_03.json","title":"...","primary_url":"..."},
    {"path":"{{OPENCLAW_HOME}}/workspace/jobs/<folder_name>/story_04.json","title":"...","primary_url":"..."},
    {"path":"{{OPENCLAW_HOME}}/workspace/jobs/<folder_name>/story_05.json","title":"...","primary_url":"..."}
  ]
}
